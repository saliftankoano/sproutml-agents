# SproutML Backend API

A sophisticated machine learning training platform that orchestrates AI agents to process datasets, perform preprocessing, and execute complete ML pipelines in isolated cloud environments.

## ğŸš€ Overview

SproutML Backend is a FastAPI-based service that provides a complete machine learning training pipeline through intelligent agent orchestration. The system leverages multiple specialized AI agents to handle different aspects of the ML workflow, from data preprocessing to model training and evaluation.

### Key Features

- **Multi-Agent Architecture**: Orchestrates specialized AI agents for different ML tasks
- **Cloud Sandbox Execution**: Runs ML pipelines in isolated Daytona cloud environments
- **Real-time Job Tracking**: Asynchronous job processing with status monitoring
- **File Management**: Secure upload, processing, and artifact retrieval
- **RESTful API**: Clean, documented endpoints for all operations

### Technology Stack

- **Framework**: FastAPI (Python 3.13+)
- **AI Agents**: Custom agent framework with OpenAI integration
- **Cloud Execution**: Daytona sandbox environments
- **File Storage**: Persistent volumes with artifact management
- **API Documentation**: Auto-generated OpenAPI/Swagger docs

## ğŸ“Š Brag Sheet

### ğŸ¯ Code Quality & Architecture Metrics

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Main.py Lines** | 519 lines | 42 lines | **92% reduction** |
| **Architecture Pattern** | Monolithic | Modular Services | **Professional-grade** |
| **Separation of Concerns** | None | 4 distinct layers | **Complete separation** |

### ğŸ—ï¸ Architectural Transformation

**From Monolithic to Microservices Architecture:**
- âœ… **Configuration Layer**: Centralized environment management
- âœ… **Service Layer**: 4 specialized services (Job, Daytona, Agent, Training)
- âœ… **Route Layer**: 5 focused API modules with clean separation
- âœ… **Main Application**: Minimal orchestration layer (42 lines)

### ğŸ¨ Design Patterns Implemented

1. **Service Layer Pattern**: Business logic separated from API concerns
2. **Repository Pattern**: Clean data access through service abstractions
3. **Factory Pattern**: Agent creation with dependency injection
4. **Router Pattern**: FastAPI APIRouter for modular endpoint organization
5. **Configuration Pattern**: Environment-based configuration management

### ğŸ“ˆ Scalability & Maintainability Achievements

- **Modularity**: Each component can be developed, tested, and deployed independently
- **Testability**: Individual services can be unit tested in isolation
- **Extensibility**: New agents, services, or routes can be added without affecting existing code
- **Team Collaboration**: Multiple developers can work on different modules simultaneously
- **Code Reusability**: Services can be imported and used across different parts of the application

### ğŸ”§ Technical Excellence

- **Error Handling**: Comprehensive exception handling across all layers
- **Type Safety**: Full type hints throughout the codebase
- **Documentation**: Auto-generated API documentation with detailed schemas
- **CORS Configuration**: Production-ready cross-origin resource sharing
- **Async Processing**: Non-blocking job execution with proper resource management

## ğŸ›ï¸ Architecture Overview

```
sproutml-back/
â”œâ”€â”€ main.py                    # Application entry point (42 lines)
â”œâ”€â”€ config.py                  # Environment configuration
â”œâ”€â”€ services/                  # Business logic layer
â”‚   â”œâ”€â”€ job_service.py         # Job management (49 lines)
â”‚   â”œâ”€â”€ daytona_service.py     # Cloud sandbox management (81 lines)
â”‚   â”œâ”€â”€ agent_service.py       # AI agent orchestration (152 lines)
â”‚   â””â”€â”€ process_training_job.py # Training pipeline (147 lines)
â””â”€â”€ routes/                    # API endpoint layer
    â”œâ”€â”€ __init__.py           # Package interface (6 lines)
    â”œâ”€â”€ training.py           # Training endpoints (69 lines)
    â”œâ”€â”€ jobs.py               # Job management endpoints (51 lines)
    â”œâ”€â”€ artifacts.py          # File management endpoints (31 lines)
    â””â”€â”€ health.py             # Health check endpoints (24 lines)
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.13+
- OpenAI API Key
- Daytona API Key

### Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set up environment variables:
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

4. Run the application:
   ```bash
   python main.py
   ```

The API will be available at `http://localhost:8000` with interactive documentation at `http://localhost:8000/docs`.

## ğŸ“š API Endpoints

### Training
- `POST /train` - Start a new ML training job
- `GET /health` - Health check endpoint

### Job Management
- `GET /jobs` - List all training jobs
- `GET /job/{job_id}` - Get specific job status

### Artifacts
- `GET /job/{job_id}/artifacts` - List job artifacts
- `GET /job/{job_id}/artifact/{filename}` - Download specific artifact

## ğŸ¤– Agent Architecture

The system uses a sophisticated multi-agent architecture:

1. **Orchestrator Agent**: Coordinates the entire ML pipeline
2. **Preprocessing Agent**: Handles data cleaning and preparation
3. **Master Trainer Agent**: Manages model training processes
4. **Evaluator Agent**: Performs model evaluation and validation
5. **Tuning Agent**: Optimizes hyperparameters

## ğŸ”„ Job Processing Flow

1. **Upload**: User uploads CSV dataset with target column specification
2. **Job Creation**: System creates persistent job record with unique ID
3. **Environment Setup**: Daytona sandbox and volume are provisioned
4. **Agent Orchestration**: Orchestrator coordinates specialized agents
5. **Pipeline Execution**: Multi-step ML pipeline runs in isolated environment
6. **Artifact Generation**: Results, models, and reports are stored
7. **Status Tracking**: Real-time job status updates throughout process

## ğŸ›¡ï¸ Security & Isolation

- **Sandboxed Execution**: All ML processing runs in isolated cloud environments
- **Persistent Storage**: Secure file storage with access controls
- **API Authentication**: Ready for token-based authentication
- **CORS Configuration**: Production-ready cross-origin policies

## ğŸ“Š Performance Features

- **Asynchronous Processing**: Non-blocking job execution
- **Resource Management**: Efficient sandbox lifecycle management
- **File Streaming**: Optimized file upload/download handling
- **Concurrent Execution**: Thread pool for parallel job processing

## ğŸ§ª Development & Testing

The modular architecture enables:
- **Unit Testing**: Each service can be tested independently
- **Integration Testing**: API endpoints can be tested in isolation
- **Mock Services**: Easy to mock external dependencies
- **Development Workflow**: Hot reloading and rapid iteration

## ğŸš€ Deployment Ready

- **Docker Support**: Containerized deployment configuration
- **Environment Configuration**: Production-ready environment management
- **Health Checks**: Comprehensive monitoring endpoints
- **Logging**: Structured logging throughout the application

## ğŸ“ˆ Future Enhancements

- **Database Integration**: Replace in-memory storage with persistent database
- **Authentication**: JWT-based user authentication
- **Monitoring**: Advanced metrics and observability
- **Scaling**: Horizontal scaling with load balancing
- **Caching**: Redis integration for improved performance

## ğŸ¤ Contributing

The modular architecture makes contribution easy:
1. Pick a specific service or route module
2. Implement your feature in isolation
3. Add comprehensive tests
4. Submit pull request with clear documentation

## ğŸ“„ License

This project is part of the SproutML platform. See LICENSE file for details.

---

**Built with â¤ï¸ using FastAPI, AI Agents, and Cloud Computing**
